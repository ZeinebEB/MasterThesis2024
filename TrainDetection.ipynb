{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29fa8d0-db4a-4ea4-926a-3afec399a5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from model.Efficientnet_Det import EfficientNetEnc_Det\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model.FFTRadNet import FFTRadNet\n",
    "from dataset.dataset import RADIal\n",
    "from dataset.encoder import ra_encoder\n",
    "from dataset.dataloader import CreateDataLoaders\n",
    "import pkbar\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from loss import pixor_loss\n",
    "from utils.evaluation import run_evaluation\n",
    "import torch.nn as nn\n",
    "import torch\n",
    " \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='FFTRadNet Training')\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('-c', '--config', default='config/config_FFTRadNet_192_56-Det.json',type=str,                            help='Path to the config file (default: config.json)')\n",
    "parser.add_argument('-r', '--resume', default=None, type=str,\n",
    "                            help='Path to the .pth model checkpoint to resume training')\n",
    "\n",
    "args = parser.parse_args()\n",
    "config = json.load(open(args.config))\n",
    "resume=args.resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3a5732-1a86-4dc3-b294-03f2f7c31247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pkbar in /opt/conda/lib/python3.8/site-packages (0.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from pkbar) (1.22.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.8/site-packages (2.0.4)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.8/site-packages (from shapely) (1.22.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from efficientnet_pytorch) (1.10.2+cu113)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->efficientnet_pytorch) (4.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pkbar\n",
    "!pip install shapely\n",
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f07990-0ef1-4318-a8cd-d521110d0f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFTRadNet_RA_192_56___Jun-07-2024___07:56:12\n",
      "===========  Dataset  ==================:\n",
      "      Mode: sequence\n",
      "      Training: 6230\n",
      "      Validation: 986\n",
      "      Test: 1035\n",
      "\n",
      "Loaded pretrained weights for efficientnet-b2\n",
      "===========  Optimizer  ==================:\n",
      "      LR: 0.0001\n",
      "      step_size: 10\n",
      "      gamma: 0.9\n",
      "      num_epochs: 100\n",
      "\n",
      "Epoch: 1/100\n",
      "1558/1558 [====================] - 1163s 746ms/step - loss: 277.7750 - class: 126.3575 - reg: 151.4176 - val_loss: 161909.4141 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 2/100\n",
      "1558/1558 [====================] - 871s 559ms/step - loss: 158.4552 - class: 41.2353 - reg: 117.2199 - val_loss: 131508.5158 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 3/100\n",
      "1558/1558 [====================] - 877s 563ms/step - loss: 133.7686 - class: 31.8688 - reg: 101.8998 - val_loss: 125395.2552 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 4/100\n",
      "1558/1558 [====================] - 896s 575ms/step - loss: 120.0929 - class: 26.6332 - reg: 93.4597 - val_loss: 123090.4806 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 5/100\n",
      "1558/1558 [====================] - 898s 576ms/step - loss: 108.7259 - class: 22.6994 - reg: 86.0264 - val_loss: 121953.9845 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 6/100\n",
      "1558/1558 [====================] - 888s 570ms/step - loss: 99.4906 - class: 19.3740 - reg: 80.1167 - val_loss: 122827.7188 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 7/100\n",
      "1558/1558 [====================] - 888s 570ms/step - loss: 92.9066 - class: 17.0217 - reg: 75.8850 - val_loss: 118783.4827 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 8/100\n",
      "1558/1558 [====================] - 895s 575ms/step - loss: 88.0504 - class: 15.5277 - reg: 72.5227 - val_loss: 121006.6512 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 9/100\n",
      "1558/1558 [====================] - 899s 577ms/step - loss: 83.5512 - class: 14.0019 - reg: 69.5493 - val_loss: 117959.2202 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 10/100\n",
      "1558/1558 [====================] - 894s 574ms/step - loss: 80.6340 - class: 12.9498 - reg: 67.6842 - val_loss: 121778.1548 - mAP: 0.0000e+00 - mAR: 0.0000e+00\n",
      "\n",
      "Epoch: 11/100\n",
      "1558/1558 [====================] - 889s 570ms/step - loss: 77.6667 - class: 12.0873 - reg: 65.5794 - val_loss: 119602.7490 - mAP: 0.6985 - mAR: 0.9345\n",
      "\n",
      "Epoch: 12/100\n",
      "1558/1558 [====================] - 896s 575ms/step - loss: 74.7129 - class: 11.2694 - reg: 63.4435 - val_loss: 122848.8909 - mAP: 0.7027 - mAR: 0.9272\n",
      "\n",
      "Epoch: 13/100\n",
      "1558/1558 [====================] - 901s 578ms/step - loss: 73.5303 - class: 10.7567 - reg: 62.7736 - val_loss: 124817.3771 - mAP: 0.6836 - mAR: 0.9367\n",
      "\n",
      "Epoch: 14/100\n",
      "1558/1558 [====================] - 899s 577ms/step - loss: 72.0522 - class: 10.2791 - reg: 61.7732 - val_loss: 123345.4284 - mAP: 0.6742 - mAR: 0.9382\n",
      "\n",
      "Epoch: 15/100\n",
      "1558/1558 [====================] - 916s 588ms/step - loss: 70.6126 - class: 9.9332 - reg: 60.6794 - val_loss: 126467.0909 - mAP: 0.7336 - mAR: 0.9281\n",
      "\n",
      "Epoch: 16/100\n",
      "1558/1558 [====================] - 903s 580ms/step - loss: 69.8047 - class: 9.6568 - reg: 60.1478 - val_loss: 124257.7258 - mAP: 0.7199 - mAR: 0.9185\n",
      "\n",
      "Epoch: 17/100\n",
      "  79/1558 [>...................] - ETA: 12:20 - loss: 68.6754 - class: 9.4813 - reg: 59.1942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558/1558 [====================] - 910s 584ms/step - loss: 66.6773 - class: 8.8800 - reg: 57.7973 - val_loss: 127555.9484 - mAP: 0.7379 - mAR: 0.9186\n",
      "\n",
      "Epoch: 20/100\n",
      " 972/1558 [===========>........] - ETA: 5:08 - loss: 66.3829 - class: 8.5567 - reg: 57.8262"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 892/1558 [==========>.........] - ETA: 5:46 - loss: 64.5773 - class: 8.1436 - reg: 56.4337"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558/1558 [====================] - 897s 575ms/step - loss: 62.8361 - class: 7.8289 - reg: 55.0071 - val_loss: 131929.8109 - mAP: 0.7946 - mAR: 0.9022\n",
      "\n",
      "Epoch: 24/100\n",
      "226/247 [=================>..] - ETA: 7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558/1558 [====================] - 902s 579ms/step - loss: 61.5872 - class: 7.4719 - reg: 54.1153 - val_loss: 135782.9500 - mAP: 0.7980 - mAR: 0.8866\n",
      "\n",
      "Epoch: 27/100\n",
      "1558/1558 [====================] - 899s 577ms/step - loss: 61.2711 - class: 7.3675 - reg: 53.9036 - val_loss: 134097.4686 - mAP: 0.8159 - mAR: 0.8990\n",
      "\n",
      "Epoch: 28/100\n",
      "1558/1558 [====================] - 899s 577ms/step - loss: 60.5254 - class: 7.2305 - reg: 53.2948 - val_loss: 133278.7395 - mAP: 0.8026 - mAR: 0.9013\n",
      "\n",
      "Epoch: 29/100\n",
      " 259/1558 [==>.................] - ETA: 11:27 - loss: 60.4148 - class: 7.0228 - reg: 53.3921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558/1558 [====================] - 909s 584ms/step - loss: 60.2163 - class: 7.0922 - reg: 53.1241 - val_loss: 134745.2949 - mAP: 0.8170 - mAR: 0.8982\n",
      "\n",
      "Epoch: 30/100\n",
      "1558/1558 [====================] - 919s 590ms/step - loss: 59.9229 - class: 6.9932 - reg: 52.9296 - val_loss: 136390.1043 - mAP: 0.7779 - mAR: 0.8973\n",
      "\n",
      "Epoch: 31/100\n",
      "1558/1558 [====================] - 911s 584ms/step - loss: 59.1082 - class: 6.8184 - reg: 52.2898 - val_loss: 137604.0186 - mAP: 0.8114 - mAR: 0.8920\n",
      "\n",
      "Epoch: 32/100\n",
      "1558/1558 [====================] - 911s 585ms/step - loss: 58.1682 - class: 6.6485 - reg: 51.5197 - val_loss: 138700.6972 - mAP: 0.8270 - mAR: 0.8912\n",
      "\n",
      "Epoch: 33/100\n",
      "1463/1558 [=================>..] - ETA: 49s - loss: 58.1142 - class: 6.6144 - reg: 51.4998"
     ]
    }
   ],
   "source": [
    "# Setup random seed\n",
    "torch.manual_seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "torch.cuda.manual_seed(config['seed'])\n",
    "\n",
    "# create experience name\n",
    "curr_date = datetime.now()\n",
    "exp_name = config['name'] + '___' + curr_date.strftime('%b-%d-%Y___%H:%M:%S')\n",
    "print(exp_name)\n",
    "\n",
    "# Create directory structure\n",
    "output_folder = Path(\"Efficientnet_DET_seq\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "(output_folder / exp_name).mkdir(parents=True, exist_ok=True)\n",
    "# and copy the config file\n",
    "with open(output_folder / exp_name / 'config.json', 'w') as outfile:\n",
    "    json.dump(config, outfile)\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize tensorboard\n",
    "writer = SummaryWriter(output_folder / exp_name)\n",
    "\n",
    "# Load the dataset\n",
    "enc = ra_encoder(geometry = config['dataset']['geometry'], \n",
    "                    statistics = config['dataset']['statistics'],\n",
    "                    regression_layer = 2)\n",
    "\n",
    "dataset = RADIal(root_dir = config['dataset']['root_dir'],\n",
    "                    statistics= config['dataset']['statistics'],\n",
    "                    encoder=enc.encode,\n",
    "                    difficult=True)\n",
    "\n",
    "train_loader, val_loader, test_loader = CreateDataLoaders(dataset,config['dataloader'],config['seed'])\n",
    "\n",
    "\n",
    "# Create the model\n",
    "# net = FFTRadNet(blocks = config['model']['backbone_block'],\n",
    "#                     mimo_layer  = config['model']['MIMO_output'],\n",
    "#                     channels = config['model']['channels'], \n",
    "#                     regression_layer = 2, \n",
    "#                     detection_head = config['model']['DetectionHead'], \n",
    "#                     segmentation_head = config['model']['SegmentationHead'])\n",
    "net = EfficientNetEnc_Det(n_channels=32, n_classes=1, detection_head=True)\n",
    "\n",
    "net.to('cuda')\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "lr = float(config['optimizer']['lr'])\n",
    "step_size = int(config['lr_scheduler']['step_size'])\n",
    "gamma = float(config['lr_scheduler']['gamma'])\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "num_epochs=int(config['num_epochs'])\n",
    "num_epochs=100\n",
    "\n",
    "\n",
    "\n",
    "print('===========  Optimizer  ==================:')\n",
    "print('      LR:', lr)\n",
    "print('      step_size:', step_size)\n",
    "print('      gamma:', gamma)\n",
    "print('      num_epochs:', num_epochs)\n",
    "print('')\n",
    "\n",
    "# Train\n",
    "startEpoch = 0\n",
    "global_step = 0\n",
    "history = {'train_loss':[],'val_loss':[],'lr':[],'mAP':[],'mAR':[]}\n",
    "best_mAP = 0\n",
    "\n",
    "freespace_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "\n",
    "# if resume:\n",
    "#     print('===========  Resume training  ==================:')\n",
    "#     dict = torch.load(resume)\n",
    "#     net.load_state_dict(dict['net_state_dict'])\n",
    "#     optimizer.load_state_dict(dict['optimizer'])\n",
    "#     scheduler.load_state_dict(dict['scheduler'])\n",
    "#     startEpoch = dict['epoch']+1\n",
    "#     history = dict['history']\n",
    "#     global_step = dict['global_step']\n",
    "#\n",
    "#     print('       ... Start at epoch:',startEpoch)\n",
    "\n",
    "\n",
    "for epoch in range(startEpoch,num_epochs):\n",
    "\n",
    "    kbar = pkbar.Kbar(target=len(train_loader), epoch=epoch, num_epochs=num_epochs, width=20, always_stateful=False)\n",
    "\n",
    "    ###################\n",
    "    ## Training loop ##\n",
    "    ###################\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs = data[0].to('cuda').float()\n",
    "        label_map = data[1].to('cuda').float()\n",
    "        \n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass, enable to track our gradient\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = net(inputs)\n",
    "\n",
    "\n",
    "        classif_loss,reg_loss = pixor_loss(outputs['Detection'], label_map,config['losses'])           \n",
    "\n",
    "\n",
    "        classif_loss *= config['losses']['weight'][0]\n",
    "        reg_loss *= config['losses']['weight'][1]\n",
    "\n",
    "\n",
    "        loss = classif_loss + reg_loss \n",
    "\n",
    "        writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "        writer.add_scalar('Loss/train_clc', classif_loss.item(), global_step)\n",
    "        writer.add_scalar('Loss/train_reg', reg_loss.item(), global_step)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        kbar.update(i, values=[(\"loss\", loss.item()), (\"class\", classif_loss.item()), (\"reg\", reg_loss.item())])\n",
    "\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    history['train_loss'].append(running_loss / len(train_loader.dataset))\n",
    "    history['lr'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "\n",
    "    ######################\n",
    "    ## validation phase ##\n",
    "    ######################\n",
    "\n",
    "    eval = run_evaluation(net,val_loader,enc,check_perf=(epoch>=10),\n",
    "                            detection_loss=pixor_loss,segmentation_loss=None,\n",
    "                            losses_params=config['losses'])\n",
    "\n",
    "    history['val_loss'].append(eval['loss'])\n",
    "    history['mAP'].append(eval['mAP'])\n",
    "    history['mAR'].append(eval['mAR'])\n",
    "\n",
    "    kbar.add(1, values=[(\"val_loss\", eval['loss']),(\"mAP\", eval['mAP']),(\"mAR\", eval['mAR'])])\n",
    "\n",
    "\n",
    "    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "    writer.add_scalar('Loss/test', eval['loss'], global_step)\n",
    "    writer.add_scalar('Metrics/mAP', eval['mAP'], global_step)\n",
    "    writer.add_scalar('Metrics/mAR', eval['mAR'], global_step)\n",
    "\n",
    "    # Saving all checkpoint as the best checkpoint for multi-task is a balance between both --> up to the user to decide\n",
    "    name_output_file = config['name']+'_epoch_detection_{:02d}_loss_{:.4f}_AP_{:.4f}_AR_{:.4f}.pth'.format(epoch, eval['loss'],eval['mAP'],eval['mAR'])\n",
    "    filename = output_folder / exp_name / name_output_file\n",
    "\n",
    "    checkpoint={}\n",
    "    checkpoint['net_state_dict'] = net.state_dict()\n",
    "    checkpoint['optimizer'] = optimizer.state_dict()\n",
    "    checkpoint['scheduler'] = scheduler.state_dict()\n",
    "    checkpoint['epoch'] = epoch\n",
    "    checkpoint['history'] = history\n",
    "    checkpoint['global_step'] = global_step\n",
    "\n",
    "    torch.save(checkpoint,filename)\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50360acf-581f-4ccd-8b2e-9033aea924ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4bf8c-1e21-413f-86c9-5fda018d8910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
